# -*- coding: utf-8 -*-
"""
Application Streamlit Cloud : comparaison SHS de la proportion de vidéos "marquées IA" entre deux ensembles.

Objectif typique :
Comparer "climatosceptique" vs "climat" sur la proportion de vidéos déclarant un contenu altéré/synthétique.

Marqueur IA principal (YouTube Data API v3) :
- videos.list(part="status") renvoie parfois status.containsSyntheticMedia (True/False).
- Si le champ est absent, l'information est inconnue (ne pas interpréter comme "non IA").

Deux modes de constitution de corpus :
- Mode "Chaînes seed" : tu fournis un CSV de chaînes avec un groupe (recommandé pour une comparaison robuste).
- Mode "Recherche" : tu pars d'une requête/hashtag via search.list (exploratoire, quota élevé, biais de ranking).

Statistiques :
- Unité d'observation recommandée : la chaîne.
- On calcule une proportion de vidéos "IA déclarée" par chaîne, puis on compare les groupes.
- Test principal : permutation au niveau des chaînes (robuste à la non-indépendance vidéo/chaîne).
- Analyse de sensibilité : borne basse (inconnu = 0), borne haute (inconnu = 1), et mode "exclure les inconnus".

Déploiement Streamlit Cloud :
- Définir YOUTUBE_API_KEY dans Secrets (Settings -> Secrets) :
  YOUTUBE_API_KEY = "TA_CLE_API"
"""

from __future__ import annotations

import os
import time
import math
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


@dataclass
class ParametresAPI:
    cle_api: str
    delai_requete: float = 0.1
    nb_tentatives: int = 5


def lire_cle_api_streamlit() -> str:
    if "YOUTUBE_API_KEY" in st.secrets:
        cle = str(st.secrets["YOUTUBE_API_KEY"]).strip()
        if cle:
            return cle
    env = os.getenv("YOUTUBE_API_KEY", "").strip()
    if env:
        return env
    raise ValueError("Clé API absente. Ajoute YOUTUBE_API_KEY dans Secrets Streamlit Cloud ou en variable d'environnement.")


@st.cache_resource(show_spinner=False)
def creer_service_youtube(cle_api: str):
    return build("youtube", "v3", developerKey=cle_api, cache_discovery=False)


def requete_robuste(appel, nb_tentatives: int = 5, pause_initiale: float = 1.0):
    pause = pause_initiale
    for tentative in range(nb_tentatives):
        try:
            return appel.execute()
        except HttpError:
            if tentative == nb_tentatives - 1:
                raise
            time.sleep(pause)
            pause *= 2


def _safe_int(x) -> Optional[int]:
    try:
        if x is None:
            return None
        return int(x)
    except Exception:
        return None


def heuristique_indice_ia(titre: str, description: str) -> int:
    """
    Indicateur exploratoire basé sur le texte. 1 = indice présent, 0 = absent.
    Ce n'est pas une preuve, seulement un signal secondaire.
    """
    t = ((titre or "") + " " + (description or "")).lower()
    mots = [
        "deepfake", "ai voice", "voice ai", "voix ia",
        "generated by ai", "généré par ia", "genere par ia",
        "midjourney", "stable diffusion", "suno", "elevenlabs", "heygen", "d-id"
    ]
    return 1 if any(m in t for m in mots) else 0


def recuperer_playlist_uploads(service, channel_id: str, params: ParametresAPI) -> str:
    req = service.channels().list(part="contentDetails", id=channel_id, maxResults=1)
    rep = requete_robuste(req, nb_tentatives=params.nb_tentatives)
    items = rep.get("items", [])
    if not items:
        raise ValueError(f"Chaîne introuvable ou non accessible : {channel_id}")
    return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]


def lister_videos_playlist(service, playlist_id: str, max_videos: int, params: ParametresAPI) -> List[str]:
    video_ids: List[str] = []
    page_token = None
    while True:
        req = service.playlistItems().list(
            part="contentDetails",
            playlistId=playlist_id,
            maxResults=min(50, max_videos - len(video_ids)),
            pageToken=page_token,
        )
        rep = requete_robuste(req, nb_tentatives=params.nb_tentatives)
        for it in rep.get("items", []):
            video_ids.append(it["contentDetails"]["videoId"])
            if len(video_ids) >= max_videos:
                return video_ids

        page_token = rep.get("nextPageToken")
        if not page_token:
            break

        if params.delai_requete > 0:
            time.sleep(params.delai_requete)

    return video_ids


def recuperer_details_videos(service, video_ids: List[str], params: ParametresAPI) -> pd.DataFrame:
    lignes: List[Dict] = []
    for i in range(0, len(video_ids), 50):
        lot = video_ids[i:i + 50]
        req = service.videos().list(
            part="snippet,status,statistics,contentDetails",
            id=",".join(lot),
            maxResults=50
        )
        rep = requete_robuste(req, nb_tentatives=params.nb_tentatives)
        for v in rep.get("items", []):
            sn = v.get("snippet", {})
            stt = v.get("statistics", {})
            cd = v.get("contentDetails", {})
            status = v.get("status", {})
            lignes.append({
                "video_id": v.get("id"),
                "channel_id": sn.get("channelId"),
                "channel_title": sn.get("channelTitle"),
                "titre": sn.get("title"),
                "description": sn.get("description"),
                "published_at": sn.get("publishedAt"),
                "category_id": sn.get("categoryId"),
                "duree_iso8601": cd.get("duration"),
                "vues": _safe_int(stt.get("viewCount")),
                "likes": _safe_int(stt.get("likeCount")),
                "nb_commentaires": _safe_int(stt.get("commentCount")),
                "containsSyntheticMedia": status.get("containsSyntheticMedia")
            })
        if params.delai_requete > 0:
            time.sleep(params.delai_requete)
    return pd.DataFrame(lignes)


def collecter_mode_chaines(
    service,
    df_chaines: pd.DataFrame,
    max_videos_par_chaine: int,
    params: ParametresAPI,
    progres_callback=None
) -> pd.DataFrame:
    corpus = []
    total = len(df_chaines)
    for i, r in df_chaines.reset_index(drop=True).iterrows():
        channel_id = str(r["channel_id"]).strip()
        groupe = str(r["groupe"]).strip()

        if progres_callback:
            progres_callback((i + 1) / max(1, total), f"Collecte chaîne {i + 1}/{total} : {channel_id}")

        try:
            uploads = recuperer_playlist_uploads(service, channel_id, params)
            video_ids = lister_videos_playlist(service, uploads, max_videos_par_chaine, params)
            if not video_ids:
                continue
            df_vid = recuperer_details_videos(service, video_ids, params)
            if df_vid.empty:
                continue
            df_vid["groupe"] = groupe
            df_vid["mode_collecte"] = "chaines_seed"
            df_vid["requete_source"] = None
            df_vid["indice_ia_texte"] = df_vid.apply(lambda x: heuristique_indice_ia(x.get("titre"), x.get("description")), axis=1)
            corpus.append(df_vid)
        except HttpError:
            continue
        except Exception:
            continue

    if not corpus:
        return pd.DataFrame()
    return pd.concat(corpus, ignore_index=True)


def collecter_mode_recherche(
    service,
    requete: str,
    max_resultats: int,
    params: ParametresAPI,
    region_code: Optional[str],
    langue: Optional[str],
    progres_callback=None
) -> pd.DataFrame:
    video_ids: List[str] = []
    page_token = None

    while len(video_ids) < max_resultats:
        if progres_callback:
            progres_callback(len(video_ids) / max(1, max_resultats), f"Recherche en cours : {len(video_ids)}/{max_resultats} vidéos")

        req = service.search().list(
            part="id",
            q=requete,
            type="video",
            maxResults=min(50, max_resultats - len(video_ids)),
            pageToken=page_token,
            regionCode=region_code,
            relevanceLanguage=langue
        )
        rep = requete_robuste(req, nb_tentatives=params.nb_tentatives)
        for it in rep.get("items", []):
            vid = it.get("id", {}).get("videoId")
            if vid:
                video_ids.append(vid)

        page_token = rep.get("nextPageToken")
        if not page_token:
            break

        if params.delai_requete > 0:
            time.sleep(params.delai_requete)

    video_ids = list(dict.fromkeys(video_ids))
    if not video_ids:
        return pd.DataFrame()

    df_vid = recuperer_details_videos(service, video_ids, params)
    if df_vid.empty:
        return df_vid

    df_vid["mode_collecte"] = "recherche"
    df_vid["requete_source"] = requete
    df_vid["indice_ia_texte"] = df_vid.apply(lambda x: heuristique_indice_ia(x.get("titre"), x.get("description")), axis=1)

    if "groupe" not in df_vid.columns:
        df_vid["groupe"] = ""

    return df_vid


def normaliser_statut_ia_declare(val) -> str:
    if pd.isna(val):
        return "inconnu"
    if val is True:
        return "oui"
    if val is False:
        return "non"
    return "inconnu"


def proportion_ia_par_chaine(df: pd.DataFrame, mode_inconnu: str) -> pd.DataFrame:
    """
    mode_inconnu :
    - "exclure" : proportion sur les vidéos où containsSyntheticMedia est renseigné
    - "basse"   : inconnu = 0
    - "haute"   : inconnu = 1
    """
    d = df.copy()

    def binaire(val):
        if pd.isna(val):
            return np.nan
        if val is True:
            return 1.0
        if val is False:
            return 0.0
        return np.nan

    d["ia_declare"] = d["containsSyntheticMedia"].apply(binaire)

    if mode_inconnu == "basse":
        d["ia_utilisee"] = d["ia_declare"].fillna(0.0)
    elif mode_inconnu == "haute":
        d["ia_utilisee"] = d["ia_declare"].fillna(1.0)
    else:
        d = d.dropna(subset=["ia_declare"]).copy()
        d["ia_utilisee"] = d["ia_declare"]

    if d.empty:
        return pd.DataFrame(columns=["channel_id", "channel_title", "groupe", "proportion_ia", "n_videos"])

    agg = d.groupby(["channel_id", "channel_title", "groupe"]).agg(
        proportion_ia=("ia_utilisee", "mean"),
        n_videos=("video_id", "count"),
        part_indice_ia_texte=("indice_ia_texte", "mean")
    ).reset_index()

    return agg


def test_permutation_par_chaine(df_prop: pd.DataFrame, n_permutations: int, graine: int = 42, progres_callback=None) -> Dict:
    dfp = df_prop.dropna(subset=["proportion_ia", "groupe"]).copy()
    groupes = [g for g in dfp["groupe"].unique().tolist() if str(g).strip() != ""]
    if len(groupes) != 2:
        raise ValueError("Le test de permutation nécessite exactement 2 groupes (colonne 'groupe').")

    g1, g2 = groupes[0], groupes[1]
    x = dfp["proportion_ia"].to_numpy(dtype=float)
    labels = dfp["groupe"].astype(str).to_numpy()

    masque1 = labels == g1
    masque2 = labels == g2
    if masque1.sum() == 0 or masque2.sum() == 0:
        raise ValueError("Chaque groupe doit contenir au moins une chaîne.")

    obs = float(x[masque1].mean() - x[masque2].mean())

    rng = np.random.default_rng(graine)
    compte = 0

    bloc = 500
    nb_blocs = int(math.ceil(n_permutations / bloc))

    for b in range(nb_blocs):
        debut = b * bloc
        fin = min(n_permutations, (b + 1) * bloc)
        for _ in range(debut, fin):
            perm = rng.permutation(labels)
            diff = float(x[perm == g1].mean() - x[perm == g2].mean())
            if abs(diff) >= abs(obs):
                compte += 1

        if progres_callback:
            progres_callback((b + 1) / max(1, nb_blocs), f"Test de permutation : {fin}/{n_permutations}")

    p_value = (compte + 1) / (n_permutations + 1)
    return {
        "difference_moyennes": obs,
        "p_value": float(p_value),
        "groupe_1": g1,
        "groupe_2": g2,
        "n_permutations": int(n_permutations)
    }


def fusionner_marquage_machine(df: pd.DataFrame, df_mm: pd.DataFrame) -> pd.DataFrame:
    """
    df_mm attendu : video_id, marquage_machine (present/absent/inconnu) et éventuellement source.
    """
    d = df.copy()
    mm = df_mm.copy()

    if "video_id" not in mm.columns or "marquage_machine" not in mm.columns:
        raise ValueError("Le fichier marquage machine doit contenir au minimum : video_id, marquage_machine")

    mm["video_id"] = mm["video_id"].astype(str).str.strip()
    mm["marquage_machine"] = mm["marquage_machine"].astype(str).str.strip().str.lower()
    mm.loc[~mm["marquage_machine"].isin(["present", "absent", "inconnu"]), "marquage_machine"] = "inconnu"

    d["video_id"] = d["video_id"].astype(str).str.strip()
    d = d.merge(mm.drop_duplicates(subset=["video_id"]), on="video_id", how="left", suffixes=("", "_mm"))
    if "marquage_machine" not in d.columns:
        d["marquage_machine"] = np.nan
    return d


def dataframe_telechargeable(df: pd.DataFrame, nom_fichier: str, cle: str):
    csv = df.to_csv(index=False, encoding="utf-8")
    st.download_button(
        label=f"Télécharger {nom_fichier}",
        data=csv.encode("utf-8"),
        file_name=nom_fichier,
        mime="text/csv",
        key=cle
    )


def interface_principale():
    st.set_page_config(page_title="YouTube IA et climat (Streamlit Cloud)", layout="wide")
    st.title("Comparaison de marqueurs IA sur YouTube : climatosceptique versus climat")

    st.write(
        "Cette application constitue un corpus de vidéos, récupère le marqueur YouTube `containsSyntheticMedia` quand il est présent, "
        "et compare des proportions entre deux ensembles. Les vidéos sans marqueur sont traitées comme une information inconnue. "
        "Pour une comparaison défendable, l’unité d’observation recommandée est la chaîne, pas la vidéo, afin d’éviter la non-indépendance."
    )

    with st.expander("Méthode et limites (à lire avant de conclure)", expanded=False):
        st.write(
            "Le mode recherche à partir d’un hashtag ou d’une requête utilise `search.list`, qui est coûteux en quota et dépend d’un classement algorithmique. "
            "Cela produit un corpus exploratoire et non un échantillon aléatoire. Le mode chaînes seed est préférable, car il fige une liste de chaînes et collecte "
            "les N dernières vidéos de chaque chaîne via la playlist uploads. Le marqueur `containsSyntheticMedia` n’est pas un détecteur universel de l’IA : "
            "il s’agit d’une divulgation/étiquette quand elle est disponible. L’absence du champ ne signifie pas absence d’IA."
        )

    try:
        cle_api = lire_cle_api_streamlit()
    except Exception as e:
        st.error(str(e))
        st.stop()

    service = creer_service_youtube(cle_api)

    st.subheader("1) Constitution du corpus")
    mode = st.radio("Choisir un mode", ["Chaînes seed (recommandé)", "Recherche par hashtag/requête (exploratoire)"], horizontal=True)

    params = ParametresAPI(
        cle_api=cle_api,
        delai_requete=float(st.slider("Délai entre requêtes (secondes)", 0.0, 1.0, 0.1, 0.05)),
        nb_tentatives=5
    )

    prog = st.progress(0)
    info = st.empty()

    def progres(val, texte):
        prog.progress(min(1.0, max(0.0, float(val))))
        info.write(texte)

    df_corpus = None

    if mode.startswith("Chaînes"):
        st.write(
            "Le CSV doit contenir au minimum deux colonnes : `channel_id` et `groupe`. "
            "Le groupe peut être par exemple `climatosceptique` ou `climat`."
        )
        fichier = st.file_uploader("Importer un CSV de chaînes seed", type=["csv"])
        max_videos = st.slider("Nombre de vidéos par chaîne", 5, 200, 50, 5)

        if fichier is not None:
            df_ch = pd.read_csv(fichier)
            if "channel_id" not in df_ch.columns or "groupe" not in df_ch.columns:
                st.error("CSV invalide. Colonnes attendues : channel_id, groupe")
            else:
                if st.button("Lancer la collecte (chaînes)", type="primary"):
                    df_ch = df_ch[["channel_id", "groupe"]].dropna().copy()
                    df_ch["channel_id"] = df_ch["channel_id"].astype(str).str.strip()
                    df_ch["groupe"] = df_ch["groupe"].astype(str).str.strip()
                    df_ch = df_ch[(df_ch["channel_id"] != "") & (df_ch["groupe"] != "")]
                    df_corpus = collecter_mode_chaines(service, df_ch, int(max_videos), params, progres_callback=progres)

    else:
        st.write(
            "La recherche est utile pour repérer des vidéos et des chaînes, mais elle est coûteuse en quota et dépend d’un classement. "
            "Tu peux ensuite étiqueter le groupe (climatosceptique/climat) dans le tableau avant l’analyse."
        )
        requete = st.text_input("Hashtag ou requête", value="#climatic")
        max_resultats = st.slider("Nombre maximal de vidéos", 20, 500, 150, 10)
        region = st.text_input("RegionCode optionnel (ex: FR, US)", value="")
        langue = st.text_input("Langue optionnelle (ex: fr, en)", value="")

        if st.button("Lancer la collecte (recherche)", type="primary"):
            region_code = region.strip() if region.strip() else None
            relevance_lang = langue.strip() if langue.strip() else None
            df_corpus = collecter_mode_recherche(service, requete.strip(), int(max_resultats), params, region_code, relevance_lang, progres_callback=progres)

    prog.progress(0)
    info.write("")

    if df_corpus is None:
        st.stop()

    if df_corpus.empty:
        st.warning("Aucune donnée collectée. Vérifie la clé API, les identifiants, la requête et les quotas.")
        st.stop()

    df_corpus["ia_declare_statut"] = df_corpus["containsSyntheticMedia"].apply(normaliser_statut_ia_declare)

    st.subheader("2) Option : enrichissement par marquage machine (import CSV)")
    st.write(
        "Si tu disposes d’une source externe indiquant un marquage machine (present/absent/inconnu), "
        "tu peux l’importer pour lier ces informations au corpus via `video_id`."
    )
    fichier_mm = st.file_uploader("Importer un CSV de marquage machine (video_id, marquage_machine)", type=["csv"], key="mm")
    if fichier_mm is not None:
        try:
            df_mm = pd.read_csv(fichier_mm)
            df_corpus = fusionner_marquage_machine(df_corpus, df_mm)
            st.success("Marquage machine fusionné au corpus.")
        except Exception as e:
            st.error(str(e))

    st.subheader("3) Vérification et étiquetage des groupes")
    st.write(
        "Si la colonne `groupe` est vide (cas fréquent en mode recherche), tu peux la compléter ici. "
        "L’analyse nécessite exactement deux groupes distincts."
    )
    colonnes_affichees = ["video_id", "channel_title", "titre", "published_at", "ia_declare_statut", "indice_ia_texte", "groupe"]
    if "marquage_machine" in df_corpus.columns:
        colonnes_affichees.insert(6, "marquage_machine")

    df_edit = st.data_editor(
        df_corpus[colonnes_affichees].copy(),
        use_container_width=True,
        num_rows="dynamic"
    )

    df_corpus = df_corpus.drop(columns=["groupe"], errors="ignore").merge(
        df_edit[["video_id", "groupe"]].astype({"video_id": str}),
        on="video_id",
        how="left"
    )

    st.subheader("4) Export du corpus")
    dataframe_telechargeable(df_corpus, "corpus_youtube_ia.csv", "dl_corpus")

    st.subheader("5) Analyse statistique (niveau chaîne)")
    st.write(
        "L’analyse calcule une proportion de vidéos IA déclarées par chaîne, puis compare les distributions entre groupes par un test de permutation. "
        "Trois lectures sont proposées : exclure les inconnus, borne basse (inconnu=0), borne haute (inconnu=1)."
    )

    n_perm = st.slider("Nombre de permutations", 1000, 50000, 5000, 1000)

    prog2 = st.progress(0)
    info2 = st.empty()

    def progres2(val, texte):
        prog2.progress(min(1.0, max(0.0, float(val))))
        info2.write(texte)

    if st.button("Lancer l'analyse", type="primary"):
        df_analyse = df_corpus.copy()
        df_analyse["groupe"] = df_analyse["groupe"].fillna("").astype(str).str.strip()

        groupes = sorted([g for g in df_analyse["groupe"].unique().tolist() if g.strip() != ""])
        if len(groupes) != 2:
            st.error("L’analyse nécessite exactement deux groupes non vides dans la colonne `groupe`.")
            st.stop()

        resultats = []
        exports = {}

        for mode_inconnu in ["exclure", "basse", "haute"]:
            df_prop = proportion_ia_par_chaine(df_analyse, mode_inconnu=mode_inconnu)
            exports[f"proportions_par_chaine_{mode_inconnu}.csv"] = df_prop

            if df_prop.empty:
                resultats.append({
                    "mode_inconnu": mode_inconnu,
                    "erreur": "Aucune donnée exploitable (par exemple trop d'inconnus en mode exclure)."
                })
                continue

            try:
                res = test_permutation_par_chaine(df_prop, int(n_perm), progres_callback=progres2)
                res["mode_inconnu"] = mode_inconnu
                res["erreur"] = ""
                resultats.append(res)
            except Exception as e:
                resultats.append({
                    "mode_inconnu": mode_inconnu,
                    "erreur": str(e)
                })

        prog2.progress(0)
        info2.write("")

        df_res = pd.DataFrame(resultats)
        st.write("Résultats du test de permutation (différence de moyennes des proportions par chaîne).")
        st.dataframe(df_res, use_container_width=True)

        dataframe_telechargeable(df_res, "tests_permutation.csv", "dl_tests_perm")

        st.write("Proportions par chaîne (exports).")
        for nom, dfx in exports.items():
            st.write(nom)
            st.dataframe(dfx, use_container_width=True)
            dataframe_telechargeable(dfx, nom, f"dl_{nom}")

        st.write(
            "Lecture recommandée : si les trois modes (exclure, basse, haute) racontent la même histoire, ton résultat est plus robuste. "
            "Si la conclusion change selon le traitement des inconnus, il faut le signaler explicitement et éviter de sur-interpréter."
        )


if __name__ == "__main__":
    interface_principale()
